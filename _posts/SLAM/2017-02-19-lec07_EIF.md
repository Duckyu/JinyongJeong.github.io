---
layout: post
title: '[SLAM] Extended Information Filter(EIF) SLAM'
tags: [SLAM]
description: >
  또 다른 종류의 filter인 Extended Information Filter(EIF) SLAM에 대해서 설명한다.
sitemap :
  changefreq : weekly
  priority : 1.0
---

**본 글은 University Freiburg의 [Robot Mapping](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/) 강의를 바탕으로 이해하기 쉽도록 정리하려는 목적으로 작성되었습니다. 개인적인 의견을 포함하여 작성되기 때문에 틀린 내용이 있을 수도 있습니다. 틀린 부분은 지적해주시면 확인 후 수정하겠습니다.**

Information Filter는 Kalman filter의 변형으로 추후 계산상의 이점을 갖기 위한 표현 방법이다. 두 표현법의 가장 직관적인 이해 방법은 두 filter의 matrix form의 의미를 이해하는 것이다. Kalman filter의 covariance matrix는 각 element간의 불확실성에 대한 정보를 표현한다. 즉 두 element들(로봇의 위치와 landmark, 혹은 landmark 끼리)의 관계가 명확할 수록 covariance matrix값은 작고, 불확실할 수록 크다. 반면에 Information filter에서 Information matrix의 값은 covariance matrix와는 반대로, 두 관계가 정확할수록 값이 크다. 즉 두 element사이 정보의 정확도를 표현하는 matrix라고 생각할 수 있다.

## Information Filter

<img align="middle" src="/images/post/SLAM/lec07_EIF/EIF_representation.png" width="100%">


위 식은 EKF와 EIF에서의 Gaussian 표현방법을 보여준다. $$\Omega = \Sigma^{-1}$$는 Information matrix라고 부르며, 각 성분간의 정보 정확성을 표현한다. $$\xi = \Sigma_{-1}\mu$$는 Information vector라고 부른다.

### Gaussian distribution in Information form

그렇다면 위의 표현방법을 이용하여 Gaussian 분포를 표현해보자.

$$
\begin{aligned}
p(x) &= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))\\
     &= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu -\frac{1}{2}\mu^T\Sigma^{-1}\mu)\\
     &= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2}\mu^T \Sigma^{-1}\mu)exp(-\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu)\\
     &= \eta exp(-\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu)\\
     &= \eta exp(-\frac{1}{2}x^T \Omega x + x^T \xi)
\end{aligned}
$$

위 식은 mean과 covariance로 표현된 Gaussian 분포로 부터 Information matrix와 vector로 표현된 Gaussian 분포를 유도하는 과정을 보여준다. 여기서 $$\eta$$는 상수를 의미한다. 상수까지 모두 표현한 Gaussian 분포는 다음과 같다.

$$
p(x) = \frac{exp(-\frac{1}{2}\mu^T \xi)}{det(2 \pi \Omega^{-1})^{\frac{1}{2}}} exp(-\frac{1}{2}x^T \Omega x + x^T \xi)
$$

### Marginalization and Conditioning

<img align="middle" src="/images/post/SLAM/lec07_EIF/margin_condi.png" width="100%">

위의 표는 covariance matrix와 information matrix가 block matrix로 표현 되었을 때, marginalization과 conditioning을 계산하는 식을 보여주고 있다. Covariance matrix 형태일 경우에 marginalization 계산식은 block matrix에서 바로 가져올 수 있으므로 간단하다. 반면 conditioning의 경우 상대적으로 복잡하며, 계산량이 많은 inverse($$\Sigma_{\beta\beta}^{-1}$$)가 포함되어 있다. Information matrix 형태의 경우 conditioning계산은 간단하지만, marginalization 계산은 상대적으로 복잡하다(이 계산 또한 inverse를 포함하고 있기 때문에). 따라서 어떤 연산을 주로 하느냐에 따라서 더 유리한 표현방법을 선택할 수 있다.

### Information Filter algorithm

Information filter 알고리즘은 Kalman filter 알고리즘에서 표현방법을 바꾼 알고리즘으로 생각하면 된다. 우선 선형 모델에서의 Kalman filter알고리즘은 아래와 같다. 비선형 모델을 고려한 Extended Information Filter(EIF)는 선형 모델을 설명 후 다루기로 한다. 아직 Kalman filter에 익숙하지 않으면 [EKF](http://jinyongjeong.github.io/2017/02/14/lec03_kalman_filter_and_EKF/)를 우선 공부하기를 추천한다.

$$
\begin{aligned}
1: & \text{Kalman filter}(\mu_{t-1}, \Sigma_{t-1}, u_t, z_t)\\
2: & \ \ \bar{\mu}_t = A_t \mu_{t-1} + B_t u_t\\
3: &\ \ \bar{\Sigma_t} = A_t \Sigma_{t-1} A_t^T + R_t\\
4: &\ \ K_t = \bar{\Sigma_t}C_t^T(C_t \bar{\Sigma_t}C_t^T + Q_t)^{-1}\\
5: &\ \ \mu_t = \bar{\mu_t} + K_t(z_t - C_t \bar{\mu_t})\\
6: &\ \ \Sigma_t = (I - K_t C_t)\bar{\Sigma_t}\\
7: &\ \ \text{return} \ \ \mu_t, \Sigma_t\\
\end{aligned}
$$

KF에서 IF로 바꾸는 과정에는 Information matrix와 vector의 정의를 이용한다.

$$
\begin{aligned}
\Omega &= \Sigma^{-1}\\
\xi &= \Sigma_{-1}\mu
\end{aligned}
$$

#### Prediction step

2,3번은 Kalman filter의 prediction step이다. Information의 정의에 의해 Information matrix는 다음과 같이 정의된다.

$$
\begin{aligned}
\bar{\Omega}_t  &= \bar{\Sigma}_t^{-1}\\
                &= (A_t \Omega_{t-1}^{-1} A_t^T + R_t)^{-1}
\end{aligned}
$$

또한 information vector는 다음과 같다.

$$
\begin{aligned}
\bar{\xi}_t &= \bar{\Sigma}_t^{-1}\bar{\mu_t}\\
            &= \bar{\Omega}_t (A_t\mu_{t-1}+B_t u_t)\\
            &= \bar{\Omega}_t (A_t \Omega_{t-1}^{-1} \xi_{t-1}+ B_t u_t)\\
\end{aligned}
$$

Information matrix와 vector를 구하는 과정은 단순히 정의를 이용하여 유도를 하는 과정이므로 어렵지 않다. 이로써 쉽게 Information Filter의 prediction 과정을 유도하였다. <font color="blue">여기서 중요한점은 KF의 경우 prediction의 계산량이 크지 않았다. 하지만 IF로 바뀌면 prediction식에 Information matrix의 inverse가 포함되어 계산량이 증가하게 된다.</font>

#### Correction step

IF의 correction step은 bayes filter의 measurement update를 이용하여 유도한다. bayes filter의 measurement update는 다음과 같다.

$$
bel(x_t) = \eta p(z_t \mid x_t) \bar{bel}(x_t)
$$

위의 식에 prediction의 Gaussian 분포와 observation model의 Gaussian 분포를 대입하여 정리한다.

$$
\begin{aligned}
bel(x_t) &= \eta p(z_t \mid x_t) \bar{bel}(x_t)\\
         &= \eta' exp(-\frac{1}{2}(z_t-C_tx_t)^TQ_t^{-1}(z_t - C_tx_t))exp(-\frac{1}{2}(x_t-\bar{\mu}_t)^{T}\bar{\Sigma}_t^{-1}(x_t-\bar{\mu}_t))\\
         &= \eta' exp(-\frac{1}{2}(z_t-C_tx_t)^TQ_t^{-1}(z_t - C_tx_t)-\frac{1}{2}(x_t-\bar{\mu}_t)^{T}\bar{\Sigma}_t^{-1}(x_t-\bar{\mu}_t))\\
         &= \eta'' exp(-\frac{1}{2} x_t^TC_t^TQ_t^{-1}C_tx_t + x_t^TC_t^TQ_t^{-1}z_t - \frac{1}{2}x_t^T\bar{\Omega}_tx_t + x_t^T\bar{\xi}_t)\\
         &= \eta'' exp(-\frac{1}{2}x_t^T [C_t^TQ_t^{-1}C_t + \bar{\Omega}_t]x_t + x_t^T [C_t^TQ_t^{-1}z_t + \bar{\xi}_t])\\
         &= \eta'' exp(-\frac{1}{2} x_t^T \Omega_t x_t + x_t^T \xi_t)
\end{aligned}
$$

bayes filter의 measurement update식으로 정리한 식과, 위에서 정리했던 Information form의 Gaussian 분포의 식을 비교해 보자. 두 식을 비교해보면 아래와 같이 correction step에서의 information matrix와 vector의 계산 식을 구해낼 수 있다.

$$
\begin{aligned}
\Omega_t &=  C_t^TQ_t^{-1}C_t + \bar{\Omega}_t \\
\xi_t &= C_t^TQ_t^{-1}z_t + \bar{\xi}_t
\end{aligned}
$$

#### Information Filter Algorithm 정리

$$
\begin{aligned}
1: & \text{Information Filter}(\xi_{t-1}, \Omega_{t-1} u_t, z_t)\\
&\text{[Prediction step]}\\
2: & \ \ \bar{\Omega}_t = (A_t \Omega_{t-1}^{-1} A_t^T + R_t)^{-1}\\
3: &\ \ \bar{\xi}_t = \bar{\Omega}_t (A_t \Omega_{t-1}^{-1} \xi_{t-1}+ B_t u_t)\\
&\text{[Correction step]}\\
4: &\ \ \Omega_t =  C_t^TQ_t^{-1}C_t + \bar{\Omega}_t\\
5: &\ \ \xi_t = C_t^TQ_t^{-1}z_t + \bar{\xi}_t\\
6: &\ \ \text{return} \ \ \xi_t, \Omega_t\\
\end{aligned}
$$

위에서 유도했던 Information filter의 과정을 정리하면 다음과 같다. 대부분의 과정은 KF와 유사하며, 다른점이 있다면 Kalman gain을 구하는 과정이 필요없다. 또한 앞에서 언급했었지만 <font color="blue">IF와 KF의 차이점은 KF의 경우 Kalman gain을 구하는 correction step에서 계산량이 많지만, IF의 경우 prediction step에서 Information matrix를 구할 때 inverse 계산때문에 계산량이 많다. </font> IF의 correction step에 있는 $$Q^{-1}$$는 센서의 uncertainty이기 때문에 미리 계산 가능함으로 계산량에 영향을 주지 않는다.

## Extended Information Filter



**본 글을 참조하실 때에는 출처 명시 부탁드립니다.**
