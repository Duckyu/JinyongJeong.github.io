---
layout: post
title: '[SLAM] Extended Information Filter(EIF) SLAM'
tags: [SLAM]
description: >
  또 다른 종류의 filter인 Extended Information Filter(EIF) SLAM에 대해서 설명한다.
sitemap :
  changefreq : weekly
  priority : 1.0
---

**본 글은 University Freiburg의 [Robot Mapping](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/) 강의를 바탕으로 이해하기 쉽도록 정리하려는 목적으로 작성되었습니다. 개인적인 의견을 포함하여 작성되기 때문에 틀린 내용이 있을 수도 있습니다. 틀린 부분은 지적해주시면 확인 후 수정하겠습니다.**

Information Filter는 Kalman filter의 변형으로 추후 계산상의 이점을 갖기 위한 표현 방법이다. 두 표현법의 가장 직관적인 이해 방법은 두 filter의 matrix form의 의미를 이해하는 것이다. Kalman filter의 covariance matrix는 각 element간의 불확실성에 대한 정보를 표현한다. 즉 두 element들(로봇의 위치와 landmark, 혹은 landmark 끼리)의 관계가 명확할 수록 covariance matrix값은 작고, 불확실할 수록 크다. 반면에 Information filter에서 Information matrix의 값은 covariance matrix와는 반대로, 두 관계가 정확할수록 값이 크다. 즉 두 element사이 정보의 정확도를 표현하는 matrix라고 생각할 수 있다.

## Information Filter

<img align="middle" src="/images/post/SLAM/lec07_EIF/EIF_representation.png" width="100%">


위 식은 EKF와 EIF에서의 Gaussian 표현방법을 보여준다. $$\Omega = \Sigma^{-1}$$는 Information matrix라고 부르며, 각 성분간의 정보 정확성을 표현한다. $$\xi = \Sigma_{-1}\mu$$는 Information vector라고 부른다.

### Gaussian distribution in Information form

그렇다면 위의 표현방법을 이용하여 Gaussian 분포를 표현해보자.

$$
\begin{aligned}
p(x) &= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu))\\
     &= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu -\frac{1}{2}\mu^T\Sigma^{-1}\mu)\\
     &= det(2 \pi \Sigma)^{-\frac{1}{2}} exp(-\frac{1}{2}\mu^T \Sigma^{-1}\mu)exp(-\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu)\\
     &= \eta exp(-\frac{1}{2}x^T\Sigma^{-1}x + x^T\Sigma^{-1}\mu)\\
     &= \eta exp(-\frac{1}{2}x^T \Omega x + x^T \xi)
\end{aligned}
$$

위 식은 mean과 covariance로 표현된 Gaussian 분포로 부터 Information matrix와 vector로 표현된 Gaussian 분포를 유도하는 과정을 보여준다. 여기서 $$\eta$$는 상수를 의미한다. 상수까지 모두 표현한 Gaussian 분포는 다음과 같다.

$$
p(x) = \frac{exp(-\frac{1}{2}\mu^T \xi)}{det(2 \pi \Omega^{-1})^{\frac{1}{2}}} exp(-\frac{1}{2}x^T \Omega x + x^T \xi)
$$

### Marginalization and Conditioning

<img align="middle" src="/images/post/SLAM/lec07_EIF/margin_condi.png" width="100%">

위의 표는 covariance matrix와 information matrix가 block matrix로 표현 되었을 때, marginalization과 conditioning을 계산하는 식을 보여주고 있다. Covariance matrix 형태일 경우에 marginalization 계산식은 block matrix에서 바로 가져올 수 있으므로 간단하다. 반면 conditioning의 경우 상대적으로 복잡하며, 계산량이 많은 inverse($$\Sigma_{\beta\beta}^{-1}$$)가 포함되어 있다. Information matrix 형태의 경우 conditioning계산은 간단하지만, marginalization 계산은 상대적으로 복잡하다(이 계산 또한 inverse를 포함하고 있기 때문에). 따라서 어떤 연산을 주로 하느냐에 따라서 더 유리한 표현방법을 선택할 수 있다.

### Information Filter algorithm

Information filter 알고리즘은 Kalman filter 알고리즘에서 표현방법을 바꾼 알고리즘으로 생각하면 된다. 우선 선형 모델에서의 Kalman filter알고리즘은 아래와 같다. 비선형 모델을 고려한 Extended Information Filter(EIF)는 선형 모델을 설명 후 다루기로 한다. 아직 Kalman filter에 익숙하지 않으면 [EKF](http://jinyongjeong.github.io/2017/02/14/lec03_kalman_filter_and_EKF/)를 우선 공부하기를 추천한다.

$$
\begin{aligned}
1: & Kalman \ \ filter(\mu_{t-1}, \Sigma_{t-1}, u_t, z_t)\\
2: & \ \ \bar{\mu}_t = A_t \mu_{t-1} + B_t u_t\\
3: &\ \ \bar{\Sigma_t} = A_t \Sigma_{t-1} A_t^T + R_t\\
4: &\ \ K_t = \bar{\Sigma_t}C_t^T(C_t \bar{\Sigma_t}C_t^T + Q_t)^{-1}\\
5: &\ \ \mu_t = \bar{\mu_t} + K_t(z_t - C_t \bar{\mu_t})\\
6: &\ \ \Sigma_t = (I - K_t C_t)\bar{\Sigma_t}\\
7: &\ \ return \ \ \mu_t, \Sigma_t\\
\end{aligned}
$$

KF에서 IF로 바꾸는 과정에는 Information matrix와 vector의 정의를 이용한다.

$$\Omega = \Sigma^{-1}$$

$$\xi = \Sigma_{-1}\mu$$

#### Prediction step

2,3번은 Kalman filter의 prediction step이다. Information의 정의에 의해 Information matrix는 다음과 같이 정의된다.

$$
\begin{aligned}
\bar{\Omega}_t  &= \bar{\Sigma}_t^{-1}\\
                &= (A_t \Omega_{t-1}^{-1} A_t^T + R_t)^{-1}
\end{aligned}
$$

또한 information vector는 다음과 같다.

$$
\begin{aligned}
\bar{\xi}_t &= \bar{\Sigma}_t^{-1}\bar{\mu_t}\\
            &= \bar{\Omega}_t (A_t\mu_{t-1}+B_t u_t)\\
            &= \bar{\Omega}_t (A_t \Omega_{t-1}^{-1} \xi_{t-1}+ B_t u_t)\\
\end{aligned}
$$

Information matrix와 vector를 구하는 과정은 단순히 정의를 이용하여 유도를 하는 과정이므로 어렵지 않다. 이로써 쉽게 Information Filter의 prediction 과정을 유도하였다. *여기서 중요한점은 KF의 경우 prediction의 계산량이 크지 않았다. 하지만 IF로 바뀌면 prediction식에 Information matrix의 inverse가 포함되어 계산량이 증가하게 된다.*

#### Correction step

IF의 correction step은 bayes filter의 measurement update를 이용하여 유도한다. bayes filter의 measurement update는 다음과 같다.

$$
bel(x_t) = \eta p(z_t \mid x_t) \bar{bel}(x_t)
$$

위의 식에 prediction의 Gaussian 분포와 observation model의 Gaussian 분포를 대입하여 정리한다.

$$
\begin{aligned}
bel(x_t) &= \eta p(z_t \mid x_t) \bar{bel}(x_t)\\
         &= \eta' exp(-\frac{1}{2}(z_t-C_tx_t)^TQ_t^{-1}(z_t - C_tx_t))exp(-\frac{1}{2}(x_t-\bar{\mu}_t)^{T}\bar{\Sigma}_t^{-1}(x_t-\bar{\mu}_t))\\
         &= \eta' exp(-\frac{1}{2}(z_t-C_tx_t)^TQ_t^{-1}(z_t - C_tx_t)-\frac{1}{2}(x_t-\bar{\mu}_t)^{T}\bar{\Sigma}_t^{-1}(x_t-\bar{\mu}_t))\\
         &= \eta'' exp(-\frac{1}{2} x_t^TC_t^TQ_t^{-1}C_tx_t + x_t^TC_t^TQ_t^{-1}z_t - \frac{1}{2}x_t^T\bar{\Omega}_tx_t + x_t^T\bar{\xi}_t)\\
         &= \eta'' exp(-\frac{1}{2}x_t^T [C_t^TQ_t^{-1}C_t + \bar{\Omega}_t]x_t + x_t^T \underbrace{[C_t^TQ_t^{-1}z_t + \bar{\xi}_t]}_{d})
\end{aligned}
$$


## Extended Information Filter


**본 글을 참조하실 때에는 출처 명시 부탁드립니다.**
