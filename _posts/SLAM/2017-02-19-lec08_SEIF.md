---
layout: post
title: '[SLAM] Sparse Extended Information Filter(SEIF) SLAM'
tags: [SLAM]
description: >
  Filter의 연산량 개선을 위한 Sparse Extended Information Filter(SEIF) SLAM에 대해서 설명한다.
sitemap :
  changefreq : weekly
  priority : 1.0
---

**본 글은 University Freiburg의 [Robot Mapping](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/) 강의를 바탕으로 이해하기 쉽도록 정리하려는 목적으로 작성되었습니다. 개인적인 의견을 포함하여 작성되기 때문에 틀린 내용이 있을 수도 있습니다. 틀린 부분은 지적해주시면 확인 후 수정하겠습니다.**

이 글에서는 Extended Information Filter(EIF)의 계산량을 줄이기 위한 방법인 Sparse Extended Information Filter(SEIF)에 대해서 설명하려고 한다. 앞의 글에서 EKF와 EIF에 대해서 설명하였다. EKF는 Gaussian 분포를 mean vector와 covariance matrix로 표현하였고, EIF는 information vector와 information matrix로 표현하였다.

<img align="middle" src="/images/post/SLAM/lec08_SEIF/EIF_representation.png" width="100%">

## Motivation of SEIF

EKF와 EIF는 모두 앞에서 설명했던것 처럼 prediction step과 correction step으로 나누어 진다. Matrix의 연산중에서 가장 많은 계산량을 요구하는 부분은 matrix의 inverse를 계산하는 부분이다. Matrix의 inverse 계산은 matrix 크기의 quadratic하게 증가한다. EKF는 correction 과정에 inverse의 계산이 필요하며, EIF는 prediction 단계에서 inverse 계산이 필요하다. 따라서 두 알고리즘 모두 순서는 다르지만 inverse의 계산이 필요하기 때문에 알고리즘의 연산 속도는 matrix의 크기에 종속적이게 된다.

알고리즘의 연산속도가 matrix의 크기에 종속적이라는 의미는 무슨 뜻일까? SLAM의 관점에서 본다면, SLAM에서 covariance matrix의 크기는 $$3+2n$$이다. 즉 landmark의 수가 많아질 수록 covariance matrix의 크기는 증가함을 의미한다. 따라서 로봇이 이동하는 범위가 넓을 수록, 즉 landmark의 수가 많을수록 filter의 속도는 느려지고, 알고리즘을 적용할 수 있는 한계가 발생하게 된다. 이러한 문제를 해결하기 위해서 사용하는 방법이 sparsification이며, 이를 적용한 EIF가 Sparse Extended Information Filter(SEIF)이다.

아래 그림은 로봇이 landmark가 있는 환경을 움직였을 때 covariance matrix와 information matrix를 보여준다. Covariance matrix는 많은 부분이 큰 값을 가지고 있다. 하지만 Information matrix는 diagonal 부분과 몇 개의 off-diagonal 부분만이 큰 값을 가지고 있음을 알수 있다(어두울수록 큰 값이다). 이러한 차이는 불확실함을 표현하는 covariance matrix와 확실한 정보를 나타내는 information matrix의 정의를 생각하면 이해할 수 있다. 하지만 information matrix의 경우 아래 그림에서 흰색에 가까운 영역도 정확히 0이 아닌 매우 작은 non-zero 값이다.

<img align="middle" src="/images/post/SLAM/lec08_SEIF/motivation.png" width="100%">

아래 그림은 information matrix와 실제 로봇, landmark의 관계를 보여주고 있다. Informatio matrix에서 가장 왼쪽 위의 $$3 \times 3$$ matrix는 로봇의 위치에 대한 matrix이며, 다른 off-diagonal 부분은 로봇-landmark 혹은 landmark-landmark의 관계를 나타낸다. Information matrix에서 그림과 같이 off-diagonal중에서 값이 큰 부분은 로봇과 landmark간의 link가 강함을 의미하며, 값이 작은 부분은 link가 약함을 의미한다.

<img align="middle" src="/images/post/SLAM/lec08_SEIF/strong_link.png" width="100%">

Information matrix에 대해 정리해보자.

* Information matrix는 노드 사이의 constraint(구속조건)/link로 해석될 수 있다.
* Link가 없다는 것은(Information에서 값이 0) conditional independent함을 의미한다.
* $$\Omega_{ij}$$는 각 노드 사이의 link 강도를 의미한다.
* Information matrix 대부분의 off-diagonal의 값들은 거의 0에 가깝다. 하지만 정학히 0은 아니다.

그렇다면 앞에서 이야기한 sparsification이란 무엇일까? sparsification은 filter의 연산량이 matrix의 크기에 종속적이지 않도록, information matrix의 non-zero off-diagonal의 수를 한정하는 방법이다.

## Information matrix의 계산 과정 및 Sparsification

#### 1. 초기상태

<img align="middle" src="/images/post/SLAM/lec08_SEIF/info_matrix1.png" width="100%">


#### 2. 첫번째 landmark 관측

<img align="middle" src="/images/post/SLAM/lec08_SEIF/info_matrix2.png" width="100%">


#### 3. 두번째 landmark 관측

<img align="middle" src="/images/post/SLAM/lec08_SEIF/info_matrix3.png" width="100%">

#### 4. 로봇 motion update

<img align="middle" src="/images/post/SLAM/lec08_SEIF/info_matrix4.png" width="100%">


#### 5. Sparsification

<img align="middle" src="/images/post/SLAM/lec08_SEIF/info_matrix5.png" width="100%">







**본 글을 참조하실 때에는 출처 명시 부탁드립니다.**
