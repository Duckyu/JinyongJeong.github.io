---
layout: post
title: '[SLAM] Extended Kalman Filter(EKF) SLAM'
tags: [SLAM]
description: >
  실제 EKF를 SLAM에 어떻게 적용시키는지에 대해서 설명한다.

sitemap :
  changefreq : weekly
  priority : 1.0
---

**본 글은 University Freiburg의 [Robot Mapping](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/) 강의를 바탕으로 이해하기 쉽도록 정리하려는 목적으로 작성되었습니다. 개인적인 의견을 포함하여 작성되기 때문에 틀린 내용이 있을 수도 있습니다. 틀린 부분은 지적해주시면 확인 후 수정하겠습니다.**

SLAM(Simultaneous Localization and Mapping) 문제를 풀기위한 방법은 크게 3개로 나눌 수 있다.

* Kalman Filter
* Particle Filter
* Graph-based

이 글에서는 첫번째 방법인 Kalman filter를 이용한 방법에 대해서 설명한다.

### EKF for online SLAM

<img align="middle" src="/images/post/SLAM/lec05_EKF_SLAM/onlie_slam.png" width="100%">

위 그림은 EKF를 이용한 online SLAM을 표현하였다. online이라는 이름은 로봇의 이전 위치는 저장하지 않고, 현재의 위치만 추정하기 때문에 붙은 이름이다. $$\mathbf{x_t}$$는 로봇의 위치, $$\mathbf{m}$$은 landmark를 의미하며, $$z$$와 $$u$$는 각각 observation과 control input이다. 이 문제를 식으로 표현하면 다음과 같다.

$$
p(\mathbf{x_t}, \mathbf{m} \mid \mathbf{z}_{1:t}, \mathbf{u}_{1:t})
$$

즉, 처음부터 현재시간 t까지의 control input과 센서로부터 얻은 observation을 알고 있을 때 현재 로봇의 위치인 $$\mathbf{x_t}$$와 맵을 구성하는 landmark인 $$\mathbf{m}$$를 구하는 문제이다.

#### state 표현방법

우선 설명의 시작에 앞서 로봇이 이동하는 공간은 2D로 한정한다. 즉 로봇의 위치는 x좌표와 y좌표, 그리고 로봇의 방향(heading)으로 정의된다. 또한 landmark는 방향 없이 x,y좌표로 정의된다. 이 설명에서는 센서로부터 얻어진 lanemark의 정보가 map상의 어떤 landmark인지는 알고 있다는 가정을 한다. 왜냐하면 센서정보를 이용하여 landmark의 correspondence를 찾는 것 또한 하나의 연구 분야이기 때문이다. 위의 식  $$p(\mathbf{x_t}, \mathbf{m} \mid \mathbf{z}_{1:t}, \mathbf{u}_{1:t})$$에서 알 수 있듯이, control input과 observation을 이용하여 우리가 추정하고자 하는 것은 로봇의 위치와 landmark의 위치이다. 따라서 EKF SLAM에서는 다음과 같이 이 두 정보를 하나의 state vector로 정의한다.

$$
\mathbf{x}_t = (x, y, \theta, m_{1,x},m_{1,y}, m_{2,x},m_{2,y},\cdots, m_{n,x},m_{n,y})
$$

vetor($$\mathbf{x}_t$$)의 앞의 3개항($$x,y,\theta$$)는 로봇의 x,y위치와 heading의 방향을 의미하며, 나머지 항은 landmark의 위치의 x,y좌표이다. 만약 landmark의 갯수가 n개 일 때 $$\mathbf{x}_t$$ vector의 크기는 $$3+2n$$이다. 이렇게 state vector가 정의되었을 때 Covariance matrix는 다음 그림과 같이 정의된다.

<img align="middle" src="/images/post/SLAM/lec05_EKF_SLAM/representation1.png" width="100%">
<img align="middle" src="/images/post/SLAM/lec05_EKF_SLAM/representation2.png" width="100%">
<img align="middle" src="/images/post/SLAM/lec05_EKF_SLAM/representation3.png" width="100%">

맨 위의 그림을 보자, $$\mu$$는 state vector를 의미하며, $$\Sigma$$는 covariance matrix이다. 노란색으로 표시된 부분은 로봇 위치에 대한 covariance이며, 파란색으로 표시된 부분은 landmark끼리에 대한 covariance이다. 초록색으로 표시된 부분은 로봇의 위치와 landmark간의 covariance를 나타낸다. 표현의 편리성을 위하여 일반적으로 맨 위의 matrix를 맨 아래와 같이 단순화 시켜서 표시한다. 즉 $$\mathbf{x}$$는 로봇의 위치에 대한 정보를 갖고 있는 $$3\times1$$ 크기의 vetor이며, $$\mathbf{m}$$는 모든 landmark의 위치 정보를 갖고 있는 $$2n\times1$$ 크기의 vector이다. $$\Sigma_{xx}$$와 $$\Sigma_{mm}$$은 각각 $$3\times3, 2n\times2n$$크기를 갖는 로봇위 위치와 landmark 위치에 대한 covariance이다.

#### prediction step

아래 그림은 로봇이 이동하였을 때 prediction step단계를 보여주고 있다. prediction step에서는 control input을 이용하여 예상되는 로봇의 위치를 추정하는 과정이다. 따라서 로봇의 위치인 $$x_R$$과 로봇의 위치에 대한 covariance인 $$\Sigma_{x_R,x_R}$$이 변한다. 또한 로봇의 위치에 대한 covariance가 변하였기 때문에 로봇과 landmark간의 부정확성을 나타내는 $$\Sigma_{x_R,m_n}$$도 변한다. 아래 그림에서 초록색으로 표시된 부분이 prediction step에서 update되는 부분이다. prediction step에서는 covariance matrix에서 가장 큰 block matrix인 landmark의 covariance($$\Sigma_{mm}$$)은 변하지 않기 때문에 계산량이 크지 않으며, 계산량은 landmark의 갯수에 선형적으로 증가한다.  

<img align="middle" src="/images/post/SLAM/lec05_EKF_SLAM/prediction.png" width="100%">

#### correction step

Correction step은 예측된 로봇의 위치에서 예상되는 센서데이터와 실제 센서데이터와의 차이를 이용하여 로봇의 위치, 그리고 landmark의 위치를 보정한다. 이 과정에서 실제 observation의 uncertainty가 state에 반영이 되며, landmark의 covariance matrix에도 영향을 준다. 따라서 correction 단계에서는 mean vector와 covariance matrix의 모든 영역이 업데이트 된다. 따라서 correction 단계의 계산량은 landmark의 숫자에 quadratic하게 증가한다. 왜냐하면 Kalman gain을 구할 때 matrix의 inverse를 구하는데, matrix inverse의 계산은 matrix 크기에 quadratic하게 증가하기 때문이다.

<img align="middle" src="/images/post/SLAM/lec05_EKF_SLAM/correction.png" width="100%">



**본 글을 참조하실 때에는 출처 명시 부탁드립니다.**
